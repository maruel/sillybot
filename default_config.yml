# This file uses the schema from
# https://pkg.go.dev/github.com/maruel/sillybot#Config

bot:
  llm:
    # Specify a "host:port" of an already running llama.cpp, llamafile or
    # py/llm.py server.
    #
    # Useful when you can't run both the LLM and the image generation models on
    # a single machine.
    #
    # See https://github.com/maruel/sillybot/blob/main/py/README.md for how
    # to run.
    remote: ""
    # Select the model from the known models in
    # https://github.com/maruel/sillybot/blob/main/default_config.yml or select
    # a new one from Hugging Face.
    #
    # Use "python" to use the embedded pytorch generator.
    #
    # Recommended small and fast model to start with. If you have a high end
    # computer, you may want to try
    # "hf:bartowski/Mistral-Nemo-Instruct-2407-GGUF/HEAD/Mistral-Nemo-Instruct-2407-Q5_K_M"
    model: hf:Qwen/Qwen2-0.5B-Instruct-GGUF/HEAD/qwen2-0_5b-instruct-q5_k_m
    # Limit the context length for high context length models. 128K context
    # length requires a ton of RAM and significantly slow down the processing.
    # The default is to use the full model's context length.
    #context_length: 0
  image_gen:
    # Specify a "host:port" of an already running py/image_gen.py server.
    #
    # Useful when you can't run both the LLM and the image generation models on
    # a single machine.
    #
    # See https://github.com/maruel/sillybot/blob/main/py/README.md for how
    # to run.
    remote: ""
    # Use "python" to use the embedded pytorch generator. The default SSD-1B
    # with LCM-LoRA takes about 4.6GiB of VRAM.
    model: ""
  settings:
    # Warning: The prompts below are highly model-specific. Optimizing a prompt
    # for one model will likely result in mediocre outcome for a different
    # model.
    #
    # Default system prompt to use. You can tell it to be funny or sarcastic.
    # Beware that if you do so, tool calling won't work as well.
    #
    # Warning: some models react in wildly different manner. For example
    # including "You reply with short answers." to Mistral-Nemo will make it
    # absolutely terse, while Llama will just not care and will still be overly
    # verbose.
    #
    # It's not required, the model will take the first user message as is.
    prompt_system: "You are an AI assistant. You reply with short answers."
    # Prompt to use to generate Stable Diffusion prompts from a short
    # description the user provides.
    #
    # This prompt works well with Mistral models but not with Llama ones.
    prompt_image: "You are autoregressive language model that specializes in creating perfect, outstanding prompts for generative art models like Stable Diffusion.\n
      Your job is to take my ideas, capture ALL main parts, and turn into amazing dense prompts.\n
      You have to capture everything from my prompt and then use your talent to make it amazing.\n
      The level of the prompt you provide will directly affect the level of detail and quality of the artwork.\n
      Reply with only keywords. Prefixing is unnecessary.\n
      You are decisive. Your reply avoid questions. I will give you all the information you need to accomplish your task.\n
      Capture the subject that I meant exactly. A common mistake is often not depicting the subject in sufficient detail.\n
      If I ask for realism, do not add an artist style, clearly specify that the image must be realistic, in 4K, optionally with bokeh.\n
      Stable Diffusion cannot read our minds. We have to say exactly what we want.\n
      Stay faithful to what I mean. Limit subjects to what I want. Limit style to what I specify.\n
      You can add adjectives to add a sense of ambiance to the image and make it rendered with more quality.\n
      In addition, we can add some keywords to control the color tone of the image, lighting effects, etc.\n
      I provide below examples of excellent keywords you can chose from, be creative and invent your own.\n
      character, person, ork, dog, cat, bunny, stardew valley, child, soccer
      ball, tree house, tall house, city, desert, beach, canteen, Death Star, Luke
      Skywalker, city-downtown with short buildings, sunset, cup, fox, bracelet,
      weapon, jungle, kitchen, dishwasher, Mad Max (movie), Dinosaur jurassic park,
      underwater world, plants, flowers, shells, creatures, plane, french revolution,
      ship, living room, pikachu, spagetti, painting, city street, bonsai, face,
      galaxy, fluids simulation, van, gear, landscape, supercar, driveway, villa,
      house, darth vader, minion, holidays, Paris, eye, eyes, Earth, mud, stone, goku,
      avatar, group chat, GUI, graph, data table, web page, computer, Moon, robot,
      office, laboratory, technical drawing, small world, lake, rainbow, jar
      hoodie, gold chains, dripping black goo, thug life, black and red and
      yellow paint, character composition in vector with white background, stained
      antique yellow copper paint, Dramatic Artwork, wearing leather jacket, is a DJ,
      in a nightclub, mixing live on stage, giant mixing table, Chicano airbrush art,
      Swagger! snake Culture, transparent, luxury, with ice fruits, with mint,
      connected with white, yellow and pink cream, backpack on his back, riding a
      motorcycle, magazine cover, photo for magazine, majestic, royal, tall , clear
      blue sky, Parisian, London, luxurious interior, on a clam sea, flying over
      planet, fighting, light walls, penthouse bedroom, dark walls, wooden panels,
      cute, freckles, made of buckskin, small, sitting in a movie theater, eating
      popcorn, watching a movie, gorgeous, ornate, polymer clay, Japanese, modern,
      perfect, at dawn, at dusk, sunny background, warm background, vibrant
      background, black, adventure, apocalyptic, dystopian, roof rack, abandoned,
      neighborhood, beautiful, burger, fries, setting on the horizon, shelves with
      detailed items in background, brown hair, blond hair, dark hair, turtleneck
      sweater, fireplace, stone, window, ceiling, dimly lit, living room, minimalist
      furniture, vaulted ceiling, huge room, floor to ceiling window with an ocean
      view, nighttime, closed, glass, honda cbr 650r, leather suit, glasses, seaweed
      anime, cartoon, cytus and deemo, realistic, anthopomorphic, arknights,
      very buff, painting illustration collage style, fine details, 4k resolution, 2D
      Vector Illustration, Art for Sublimation, Design Art, Chrome Art, Painting and
      Stunning Artwork, Highly Detailed Digital Painting, Airbrush Art, Highly
      Detailed Digital Artwork, digital airbrush art, detailed by Mark Brooks,
      futuristic, a masterpiece, fine details, highly detailed digital art,
      hyperrealism, highly detailed, insanely detailed, intricate, depth of field, 4K
      Commercial Food, YouTube Video Screenshot, isometric, soft colors, 20mm lens, 3d
      blender render, High quality 8K, photo realism, hyper realism, hyper detailed
      ultra intricate overwhelming realism, High Speed MO Photography, high detail,
      sharp focus, low poly, go pro footage, realistic painting, by Charles Gregory
      Artstation and Antonio Jacobsen and Edward Moran, (long shot), cinematic, fine
      carvings, fantasy, medieval, detailed post processing, unreal engineered, unreal
      5, digital texture painting, impressionist, Japanese cartoon style, octane
      render, daz, artstation, epic, concept art, matte, by artgerm, pixar,
      mark ryden, hayao miyazaki, industrial age, collage-style illustration, dream
      cinematic lighting, studio lighting, increase brightness, volumetric
      lighting, realistic shaded lighting, cozy indoor lighting, dynamic lighting,
      cozy indoor lighting, soft smooth lighting, diffuse lighting, dark, bleak, soft
      light, macro, depth of field, blur, light effect, hyper detail\n
      My next message is the prompt for the image and labels that will be displayed over the image.
      "
    # Prompt to generate meme labels.
    prompt_labels: "You are autoregressive language model that specializes in creating perfect, dense, outstanding meme text. Your job is to take user ideas, capture ALL main parts, and turn into amazing snarky meme labels. You have to capture everything from the user's prompt and then use your talent to make it amazing filled with sarcasm. Respond only with the new meme text. Make it as succinct as possible. Use few words. Use exactly one comma. Exclude article words."


# You can remove this section. The one embedded in
# https://github.com/maruel/sillybot/blob/main/default_config.yml will be used
# automatically.
knownllms:
  # Gemma 2 family:
  # https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315
  - source: hf:bartowski/gemma-2-9b-it-GGUF/HEAD/gemma-2-9b-it-
    packagingtype: gguf
    upstream: hf:google/gemma-2-9b-it
    # https://ai.google.dev/gemma/docs/formatting
    #prompt_encoding:
    #  begin_of_text: ""
    #  system_token_start: "<start_of_turn>user\n"
    #  system_token_end: "<end_of_turn>\n"
    #  user_token_start: "<start_of_turn>user\n"
    #  user_token_end: "<end_of_turn>\n"
    #  assistant_token_start: "<start_of_turn>model\n"
    #  assistant_token_end: "<end_of_turn>\n"
  - source: hf:bartowski/gemma-2-27b-it-GGUF/HEAD/gemma-2-27b-it-
    packagingtype: gguf
    upstream: hf:google/gemma-2-27b-it

  # Llama 3 / 3.1 / 3.2 family:
  # https://huggingface.co/collections/meta-llama/llama-32-66f448ffc8c32f949b04c8cf
  # https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f
  # https://huggingface.co/collections/meta-llama/meta-llama-3-66214712577ca38149ebb2b6
  - source: hf:bartowski/Llama-3.2-1B-Instruct-GGUF/HEAD/Llama-3.2-1B-Instruct-
    packagingtype: gguf
    upstream: hf:meta-llama/Meta-Llama-3.2-1B-Instruct
  - source: hf:lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/HEAD/Meta-Llama-3.1-8B-Instruct-
    packagingtype: gguf
    upstream: hf:meta-llama/Meta-Llama-3.1-8B-Instruct
    # https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/
    #prompt_encoding:
    #  begin_of_text: "<|begin_of_text|>"
    #  system_token_start: "<|start_header_id|>system<|end_header_id|>\n"
    #  system_token_end: "<end_of_turn>\n"
    #  user_token_start: "<|start_header_id|>user<|end_header_id|>\n"
    #  user_token_end: "<|eot_id|>\n"
    #  assistant_token_start: "<|start_header_id|>assistant<|end_header_id|>\n"
    #  assistant_token_end: "<|eot_id|>\n"
    # TODO: This repo put Q5_K_L and higher into subdirectories, which is
    # unsupported.
  - source: hf:bartowski/Meta-Llama-3-70B-Instruct-GGUF/HEAD/Meta-Llama-3-70B-Instruct-
    packagingtype: gguf
    upstream: hf:meta-llama/Meta-Llama-3-70B-Instruct

  # Mistral family
  # https://huggingface.co/mistralai
  - source: hf:bartowski/Mistral-7B-Instruct-v0.3-GGUF/HEAD/Mistral-7B-Instruct-v0.3-
    packagingtype: gguf
    upstream: hf:mistralai/Mistral-7B-Instruct-v0.3
    # https://docs.mistral.ai/guides/tokenization/#v3-tokenizer
    # https://github.com/mistralai/mistral-common/blob/main/src/mistral_common/tokens/tokenizers/base.py
    # https://github.com/mistralai/mistral-common/blob/main/src/mistral_common/tokens/tokenizers/sentencepiece.py
    prompt_encoding:
      begin_of_text:                ""
      system_token_start:           "[INST]\u2581"
      system_token_end:             " [/INST]"
      user_token_start:             "[INST]\u2581"
      user_token_end:               "[/INST]"
      assistant_token_start:        ""
      assistant_token_end:          "</s>"
      tools_available_token_start:  "[AVAILABLE_TOOLS]\u2581"
      tools_available_token_end:    "[/AVAILABLE_TOOLS]"
      tool_call_token_start:        "[TOOL_CALLS]\u2581"
      tool_call_token_end:          "</s>"
      tool_call_result_token_start: "[TOOL_RESULTS]\u2581"
      tool_call_result_token_end:   "[/TOOL_RESULTS]"
  - source: hf:bartowski/Mistral-Nemo-Instruct-2407-GGUF/HEAD/Mistral-Nemo-Instruct-2407-
    packagingtype: gguf
    upstream: hf:mistralai/Mistral-Nemo-Instruct-2407
    prompt_encoding:
      begin_of_text:                ""
      system_token_start:           "[INST]\u2581"
      system_token_end:             " [/INST]"
      user_token_start:             "[INST]\u2581"
      user_token_end:               "[/INST]"
      assistant_token_start:        ""
      assistant_token_end:          "</s>"
      tools_available_token_start:  "[AVAILABLE_TOOLS]\u2581"
      tools_available_token_end:    "[/AVAILABLE_TOOLS]"
      tool_call_token_start:        "[TOOL_CALLS]\u2581"
      tool_call_token_end:          "</s>"
      tool_call_result_token_start: "[TOOL_RESULTS]\u2581"
      tool_call_result_token_end:   "[/TOOL_RESULTS]"

  # Phi-3/3.1 family
  # https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3
  - source: hf:bartowski/Phi-3.1-mini-4k-instruct-GGUF/HEAD/Phi-3.1-mini-4k-instruct-
    packagingtype: gguf
    upstream: hf:microsoft/Phi-3-mini-4k-instruct
    # https://huggingface.co/microsoft/Phi-3-mini-4k-instruct#chat-format
    #prompt_encoding:
    #  begin_of_text: ""
    #  system_token_start: "<|system|>\n"
    #  system_token_end: "<|end|>\n"
    #  user_token_start: "<|user|>\n"
    #  user_token_end: "<|end|>\n"
    #  assistant_token_start: "<|assistant|>\n"
    #  assistant_token_end: "<|end|>\n"
  - source: hf:bartowski/Phi-3.1-mini-128k-instruct-GGUF/HEAD/Phi-3.1-mini-128k-instruct-
    packagingtype: gguf
    upstream: hf:microsoft/Phi-3-mini-128k-instruct
  # Nobody uploaded Phi-3-small-* gguf files. Weird.
  - source: hf:bartowski/Phi-3-medium-4k-instruct-GGUF/HEAD/Phi-3-medium-4k-instruct-
    packagingtype: gguf
    upstream: hf:microsoft/Phi-3-medium-4k-instruct
  - source: hf:bartowski/Phi-3-medium-128k-instruct-GGUF/HEAD/Phi-3-medium-128k-instruct-
    packagingtype: gguf
    upstream: hf:microsoft/Phi-3-medium-128k-instruct

  # Qwen 2.5 family
  # https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e
  # https://huggingface.co/collections/Qwen/qwen25-coder-66eaa22e6f99801bf65b0c2f
  - source: hf:Qwen/Qwen2.5-0.5B-Instruct-GGUF/HEAD/qwen2.5-0.5b-instruct-
    packagingtype: gguf
    upstream: hf:Qwen/Qwen2.5-0.5B-Instruct
    # https://github.com/QwenLM/Qwen/blob/main/tokenization_note.md#special-tokens
    #prompt_encoding:
    #  begin_of_text: ""
    #  system_token_start: "<|im_start|>\n"
    #  system_token_end: "<|im_end|>\n"
    #  user_token_start: "<|im_start|>\n"
    #  user_token_end: "<|im_end|>\n"
    #  assistant_token_start: ""
    #  assistant_token_end: "<|end_of_text|>\n"
  - source: hf:Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF/HEAD/qwen2.5-coder-0.5b-instruct-
    packagingtype: gguf
    upstream: hf:Qwen/Qwen2.5-Coder-0.5B-Instruct
  - source: hf:Qwen/Qwen2.5-1.5B-Instruct-GGUF/HEAD/qwen2.5-1.5b-instruct-
    packagingtype: gguf
    upstream: hf:Qwen/Qwen2.5-1.5B-Instruct
  - source: hf:Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF/HEAD/qwen2.5-coder-1.5b-instruct-
    packagingtype: gguf
    upstream: hf:Qwen/Qwen2.5-Coder-1.5B-Instruct
  - source: hf:Qwen/Qwen2.5-7B-Instruct-GGUF/HEAD/qwen2.5-7b-instruct-
    packagingtype: gguf
    upstream: hf:Qwen/Qwen2.5-7B-Instruct
  - source: hf:Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/HEAD/qwen2.5-coder-7b-instruct-
    packagingtype: gguf
    upstream: hf:Qwen/Qwen2.5-Coder-7B-Instruct
  - source: hf:Qwen/Qwen2.5-Coder-14B-Instruct-GGUF/HEAD/qwen2.5-coder-14b-instruct-
    packagingtype: gguf
    upstream: hf:Qwen/Qwen2.5-Coder-14B-Instruct
  - source: hf:Qwen/Qwen2.5-Coder-32B-Instruct-GGUF/HEAD/qwen2.5-coder-32b-instruct-
    packagingtype: gguf
    upstream: hf:Qwen/Qwen2.5-Coder-32B-Instruct
